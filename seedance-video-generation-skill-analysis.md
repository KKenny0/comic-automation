# Seedance Video Generation Skill 分析报告

> 目标：从“是否值得用于**解说漫自动生成**”角度，按 8 个维度完整评估该 Skill。

---

## 1. 基本信息与声明（它能解决什么问题）

| 项目 | 内容 |
|---|---|
| Skill 名称 | `seedance-video-generation` |
| 来源 | ClawHub（JackyCSer） |
| 核心能力 | 基于 Volcengine Ark API 调用 Seedance 模型，完成视频生成与任务管理 |
| 适用场景 | 文本生视频（T2V）、图片生视频（I2V）、多镜头连续生成、任务追踪 |
| 必要前置 | `ARK_API_KEY` 环境变量 |

**它主要解决的问题：**
- 把 Seedance 的 API 调用封装成可执行命令，降低调用复杂度。
- 把“异步视频任务”标准化为：创建任务 → 轮询状态 → 成功下载。
- 支持本地图片自动转 base64，减少 I2V 前置处理成本。
- 具备任务查询、筛选、删除能力，便于批量生产时运维。

---

## 2. 核心功能与工作流程（附真实示例）

### 2.1 实际能做什么（主要能力）

- 文本生成视频（T2V）
- 图片生成视频（I2V）
  - 首帧驱动
  - 首帧 + 末帧约束
  - 参考图模式（Lite I2V，1-4 张）
- 草稿模式（`draft=true`）低成本预览
- 草稿任务转正式片（`--draft-task-id`）
- 连续镜头衔接（`return_last_frame`）
- 自动音频生成（仅 Seedance 1.5 Pro，`generate_audio=true`）
- 任务管理
  - `status` 查询
  - `list` 列表筛选
  - `delete` 取消/删除

### 2.2 详细执行流程（工具链拆解）

| 阶段 | 调用工具/方式 | 说明 |
|---|---|---|
| 输入阶段 | Shell + Python CLI | 通过 `python3 seedance.py create ...` 接收参数 |
| 预处理阶段 | Python 本地逻辑 | 本地图像校验、大小检查、转 data URL(base64) |
| 创建任务 | HTTP API | `POST /contents/generations/tasks` |
| 轮询等待 | HTTP API + Python 循环 | `GET /contents/generations/tasks/{task_id}`（默认 15s） |
| 成功落地 | Python 下载 | 读取 `video_url` 保存本地文件 |
| 生命周期管理 | HTTP API | `status/list/delete` 管理任务历史 |

**结论：**
- 主要依赖 **Shell + Python + API**。
- 不依赖浏览器自动化。
- 可与 OpenClaw 的 `message` 工具组合，实现“生成后自动发送到 IM（如飞书）”。

### 2.3 真实示例 Prompt（可直接用）

1) **解说漫开场镜头（T2V）**

```text
用 seedance-video 生成一个 9:16 开场镜头：
“夜晚城市俯拍，镜头推进到主角窗前，悬疑氛围，漫画渲染风格”。
时长 5 秒，先 draft=true，确认后出 720p 正式版。
```

2) **角色状态过渡（首帧+末帧）**

```text
我会提供首帧图和末帧图，请用 seedance-video 生成角色从“惊讶”到“坚定点头”的过渡镜头。
ratio=adaptive，duration=5，wait 并下载，返回 task_id。
```

3) **分镜连续生成（解说漫批量）**

```text
按我给的 6 条分镜逐条生成：每条先 480p draft，再 720p final；
每条都 return_last_frame=true，并把上一条的 last_frame 用作下一条 first_frame，确保角色和镜头连续。
```

---

## 3. 优缺点总结

### 3.1 亮点（比同类技能强在哪里）

- **工程闭环完整**：不仅能“发请求”，还能“查状态、下载结果、管理任务”。
- **多输入模式完整**：覆盖 T2V、首帧、首尾帧、参考图。
- **连续镜头能力实用**：`return_last_frame` 对剧情连续性有价值。
- **参数可控性高**：ratio、duration、resolution、seed、service_tier 等可调。
- **适合批量生产**：有 CLI，容易接入脚本流水线。

### 3.2 槽点（痛点、繁琐点）

- **不是一站式“解说漫工厂”**：不包含自动分镜、字幕、剪辑；虽支持环境音频生成，但不等同于可控“旁白配音（TTS）”。
- **文档维护有轻微不一致**：版本号/路径描述存在差异。
- **导演成本仍在用户侧**：Prompt 与分镜质量决定上限。
- **批量编排需自建外层逻辑**：多镜头流程需要再包一层 orchestration。

### 3.3 与当前需求匹配度（解说漫自动生成）

| 需求子项 | 匹配度 | 说明 |
|---|---:|---|
| 单镜头视频生成 | 9/10 | 能力成熟，参数灵活 |
| 多镜头连续性 | 8/10 | `last_frame` 衔接可用 |
| 全自动成片（脚本→成片） | 6/10 | 缺“可控旁白TTS + 字幕 + 拼接”一体化（1.5 Pro 仅提供生成音频能力） |
| 批量生产效率 | 8/10 | CLI + 任务管理适合流程化 |
| 总体 | **7.8/10** | 适合作为“核心生成引擎” |

---

## 4. 改进与自定义建议

### 4.1 优先改进建议

- **新增 storyboard 批处理入口（高优先）**
  - 输入：分镜 JSON（镜头、旁白、时长、转场）
  - 输出：任务队列 + 状态汇总 + 文件清单

- **新增角色/画风锁定模板（高优先）**
  - 统一角色设定词、色彩关键词、镜头语言，减少漂移。

- **增加自动后处理（中优先）**
  - ffmpeg 自动拼接、加 BGM、烧录字幕、导出封面。

- **增加失败重试与预算策略（中优先）**
  - draft 先行，不达标不升正式。
  - 失败自动重试 N 次并记录原因。

### 4.2 解说漫最佳实践（交互方式）

- 先让 Agent 产出“分镜表”，确认后再生成视频。
- 每镜头两阶段：`draft -> final`，降低返工成本。
- 开启 `return_last_frame=true`，逐镜接力保持连续性。
- 每条任务保留 `task_id + 输出路径 + prompt 版本` 便于复盘。

---

## 5. 风险与注意事项

- 需要合法可用的 `ARK_API_KEY`。
- 视频 URL 有时效，建议生成后立即下载归档。
- 长视频建议拆镜头，不建议一次生成整片。
- 生产环境建议增加日志与失败重试，避免人工盯任务。

---

## 6. 是否推荐安装（明确结论）

> **结论：✅ 推荐安装。**

**推荐理由：**
- 它在“视频生成核心引擎”层面足够强，尤其适合解说漫的镜头化生产。
- 技术链路清晰、可脚本化、可批量化，便于后续接入自动流程。
- 虽非完整自动化工作室，但作为底座能力非常合格，投入产出比高。

**一句话判断：**
> 如果你的目标是“把解说漫生产自动化到 70%-80%”，这个 Skill 值得装；再补一层分镜/配音/剪辑编排即可。